{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "selenium模拟浏览器爬虫\n",
    "代理url：http://www.kuaidaili.com/\n",
    "'''\n",
    "from selenium import webdriver\n",
    "class Item(object):\n",
    "    '''\n",
    "        item类 表示每一个爬到的代理\n",
    "        模拟Scrapy框架\n",
    "        需要的代理当中的数据\n",
    "    '''\n",
    "    # ip地址\n",
    "    ip = None\n",
    "    # 端口\n",
    "    port = None \n",
    "    # http 还是 https\n",
    "    type = None\n",
    "    # 物理地址\n",
    "    local = None\n",
    "    # 速度\n",
    "    speed = None\n",
    "    \n",
    "class GetProxy(object):\n",
    "    '''\n",
    "        获取代理的类\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            初始化整个类\n",
    "        '''\n",
    "        self.starturl = 'http://www.kuaidaili.com/free/inha/'\n",
    "        # urls列表\n",
    "        self.urls = self.get_urls()\n",
    "        # 爬取的代理列表\n",
    "        self.proxy_list = self.get_proxy_list(self.urls)\n",
    "        # 文件名\n",
    "        self.filename = 'proxy.txt'\n",
    "        # 保存\n",
    "        self.saveFile(self.filename, self.proxy_list)\n",
    "        \n",
    "    def get_urls(self):\n",
    "        '''\n",
    "            返回一个代理url的列表\n",
    "        '''\n",
    "        urls = []\n",
    "        # 来两页\n",
    "        for i in range(1, 3):\n",
    "            print(i)\n",
    "            url = self.starturl + str(i)\n",
    "            urls.append(url)\n",
    "        return urls\n",
    "    def get_proxy_list(self, urls):\n",
    "        '''\n",
    "            返回抓取到代理的列表\n",
    "            整个爬虫的关键位置\n",
    "        '''\n",
    "        # 建立对象\n",
    "        browser = webdriver.PhantomJS()\n",
    "        proxy_list = []\n",
    "        \n",
    "        for url in urls:\n",
    "            browser.get(url)\n",
    "            browser.implicitly_wait(3)\n",
    "            # 找出网页代理地址table的位置\n",
    "            elements = browser.find_elements_by_xpath('//tbody/tr')\n",
    "            for element in elements:\n",
    "                # Item对象\n",
    "                item = Item()\n",
    "                item.ip = element.find_element_by_xpath('./td[1]').text\n",
    "                item.port = element.find_element_by_xpath('./td[2]').text\n",
    "                item.anonymous = element.find_element_by_xpath('./td[3]').text\n",
    "                item.local = element.find_element_by_xpath('./td[4]').text\n",
    "                item.speed = element.find_element_by_xpath('./td[5]').text\n",
    "                proxy_list.append(item)\n",
    "        # 退出\n",
    "        browser.quit()\n",
    "        return proxy_list\n",
    "    def saveFile(self, filename, proxy_list):\n",
    "        '''\n",
    "            将爬取到的结果写入本地\n",
    "        '''\n",
    "        with open(filename, 'w') as f:\n",
    "            for item in proxy_list:\n",
    "                f.write(item.ip + '\\t')\n",
    "                f.write(item.port + '\\t')\n",
    "                f.write(item.anonymous + '\\t')\n",
    "                f.write(item.local + '\\t')\n",
    "                f.write(item.speed + '\\t\\n')\n",
    "if __name__ == '__main__':\n",
    "    Get = GetProxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
